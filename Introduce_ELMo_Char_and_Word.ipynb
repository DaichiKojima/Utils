{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduce_ELMo_Char_and_Word.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksZ4o1AdCH6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !ls drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GlemZjJCQbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install janome\n",
        "!pip install overrides\n",
        "!git clone https://github.com/HIT-SCIR/ELMoForManyLangs.git\n",
        "!sudo python 'ELMoForManyLangs/setup.py' install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihHw1KdWCQXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding:utf-8\n",
        "import re\n",
        "import time\n",
        "\n",
        "import h5py\n",
        "import janome\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ELMoForManyLangs.elmoformanylangs import Embedder\n",
        "from gensim.models import KeyedVectors\n",
        "from janome.tokenizer import Tokenizer\n",
        "from keras.preprocessing import text, sequence\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from overrides import overrides\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torch.utils.data import TensorDataset, Dataset\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "DATA_ROOT = 'drive/My Drive/ELMo_allenlp_tutorial/'\n",
        "word_model_path = \"drive/My Drive/ELMo(MeCab+NEologd,大規模日本語ビジネスニュースコーパス)/単語単位埋め込みモデル\"\n",
        "char_model_path = \"drive/My Drive/ELMo(MeCab+NEologd,大規模日本語ビジネスニュースコーパス)/文字単位・単語単位埋め込みモデル\"\n",
        "fasttext_embedding_path = DATA_ROOT + 'entity_vector/entity_vector.model.bin'\n",
        "embedding_model = KeyedVectors.load_word2vec_format(fasttext_embedding_path, binary=True)\n",
        "is_char = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWqduuOGj_y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ELMoNet(nn.Module):\n",
        "      def __init__(self, elmo_model, embedding_matrix, OUTPUT_DIM):\n",
        "            super(ELMoNet, self).__init__()\n",
        "\n",
        "            embed_size = embedding_matrix.shape[1]\n",
        "            self.embedding = nn.Embedding(max_features, embed_size)\n",
        "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            self.embedding_dropout = nn.Dropout(0.1)\n",
        "            \n",
        "            self.LSTM_UNITS = 128\n",
        "            self.DENSE_HIDDEN_UNITS = self.LSTM_UNITS * 4\n",
        "            \n",
        "            self.elmo_embedder = elmo_model\n",
        "\n",
        "            self.lstm1 = nn.LSTM(1024+200, self.LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "            # self.lstm1 = nn.LSTM(200, self.LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "            self.lstm2 = nn.LSTM(self.LSTM_UNITS * 2, self.LSTM_UNITS, bidirectional=True, batch_first=True)\n",
        "\n",
        "            self.linear1 = nn.Linear(self.DENSE_HIDDEN_UNITS, self.DENSE_HIDDEN_UNITS)\n",
        "            self.dropout = nn.Dropout(0.2)\n",
        "            self.linear2 = nn.Linear(self.DENSE_HIDDEN_UNITS, OUTPUT_DIM)\n",
        "\n",
        "\n",
        "      def forward(self, x):\n",
        "            \n",
        "            l = x.shape[1]\n",
        "          \n",
        "            h_embedding = self.embedding(x)\n",
        "            h_embedding = self.embedding_dropout(h_embedding)\n",
        "        \n",
        "            \n",
        "            # 0 padding を除く\n",
        "            x_p = [[i for i in y if i !=0] for y in x.cpu().detach().numpy()]\n",
        "        \n",
        "            # x_p : index, sentences : text\n",
        "            sentences =  list(map(sequence_to_text, x_p))\n",
        "            \n",
        "            h_elmo = self.elmo_embedder.sents2elmo(sentences)\n",
        "            h_elmo = [np.concatenate(\n",
        "                [i, [[0] * 1024] * (l - len(i))], axis=0) if len(i) != l else i for i in h_elmo]\n",
        "            h_elmo = torch.tensor(h_elmo).float().cuda()\n",
        "            \n",
        "          \n",
        "            # fasttext vector と elmo vector を concat する.\n",
        "            h_embcat = torch.cat([h_elmo, h_embedding], 2)\n",
        "            \n",
        "            # h_embcat = h_embedding\n",
        "\n",
        "\n",
        "            h_lstm1, _ = self.lstm1(h_embcat)\n",
        "            h_lstm2, _ = self.lstm2(h_lstm1)\n",
        "\n",
        "            # global average pooling\n",
        "            avg_pool = torch.mean(h_lstm2, 1)\n",
        "            # global max pooling\n",
        "            max_pool, _ = torch.max(h_lstm2, 1)\n",
        "\n",
        "            h_conc = torch.cat((max_pool, avg_pool), 1)\n",
        "            out  = F.relu(self.linear1(h_conc))\n",
        "            out  = self.linear2(self.dropout(out))\n",
        "\n",
        "            return F.log_softmax(out) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5MjMdutpbjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self,X,y=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return [self.X[idx],self.y[idx]]\n",
        "        return self.X[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-_0ZQp2RSTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_labels(y):\n",
        "    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n",
        "    values = np.array(y)\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(values)\n",
        "\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "    y = onehot_encoded\n",
        "    return y, label_encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWHGZaO_RbnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text): \n",
        "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
        "  \n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)        \n",
        "    x = x.replace('\\n', '') # 改行削除\n",
        "    x = x.replace('\\t', '') # タブ削除\n",
        "    x = x.replace('年', '')\n",
        "    x = x.replace('月', '')\n",
        "    x = x.replace('日', '')\n",
        "    x = x.replace('時', '')\n",
        "    x = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', x) \n",
        "    x = re.sub(r'\\[math\\]', ' LaTex math ', x) # LaTex削除\n",
        "    x = re.sub(r'\\[\\/math\\]', ' LaTex math ', x) # LaTex削除\n",
        "    x = re.sub(r'\\\\', ' LaTex ', x) # LaTex削除   \n",
        "    x = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', x) # タグの削除\n",
        "    x = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', x) # タグの削除\n",
        "    x = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', x) # タグの削除 \n",
        "    x = re.sub(' +', ' ',x) # 連続して出現する空白の削除   \n",
        "    return x\n",
        "  \n",
        "\n",
        "def rm_puncts(text):\n",
        "    puncts = r',.\":)・《》「」『』！(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√。【】〜'\n",
        "    for punct in puncts:\n",
        "        text = text.replace(punct, '')\n",
        "    return text \n",
        "  \n",
        "  \n",
        "def rm_spaces(text):\n",
        "    spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u2009', '\\u2028', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\u3000', '\\x10', '\\x7f', '\\x9d', '\\xad',\n",
        "              '\\x97', '\\x9c', '\\x8b', '\\x81', '\\x80', '\\x8c', '\\x85', '\\x92', '\\x88', '\\x8d', '\\x80', '\\x8e', '\\x9a', '\\x94', '\\xa0', \n",
        "              '\\x8f', '\\x82', '\\x8a', '\\x93', '\\x90', '\\x83', '\\x96', '\\x9b', '\\x9e', '\\x99', '\\x87', '\\x84', '\\x9f',\n",
        "             ]\n",
        "    for space in spaces:\n",
        "            text = text.replace(space, ' ')\n",
        "    return text\n",
        "  \n",
        "\n",
        "def replace_num(text):\n",
        "    text = re.sub('[0-9]{5,}', '#####', text)\n",
        "    text = re.sub('[0-9]{4}', '####', text)\n",
        "    text = re.sub('[0-9]{3}', '###', text)\n",
        "    text = re.sub('[0-9]{2}', '##', text)\n",
        "    text = re.sub('[０-９]{5,}', '#####', text)\n",
        "    text = re.sub('[０-９]{4}', '####', text)\n",
        "    text = re.sub('[０-９]{3}', '###', text)\n",
        "    text = re.sub('[０-９]{2}', '##', text)\n",
        "    return text\n",
        "  \n",
        "  \n",
        "def preprocess(df_col):\n",
        "    df_col = df_col.astype(str).progress_apply(lambda x: rm_puncts(x))\n",
        "    df_col = df_col.astype(str).progress_apply(lambda x: rm_spaces(x))\n",
        "    df_col = df_col.astype(str).progress_apply(lambda x: clean_text(x))\n",
        "    df_col = df_col.astype(str).progress_apply(lambda x: replace_num(x))\n",
        "    df_col = df_col.astype(str).progress_apply(lambda x: tokenizer(x))\n",
        "    return df_col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbJPtdIySzZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(Path(DATA_ROOT) / \"livedoor_news_text.csv\")\n",
        "print(train.shape)\n",
        "\n",
        "j_t = Tokenizer()\n",
        "\n",
        "x_train = preprocess(train['news'])    \n",
        "y_train, le = prepare_labels(train['class'])\n",
        "\n",
        "tokenizer_text = text.Tokenizer()\n",
        "tokenizer_text.fit_on_texts(list(x_train))\n",
        "\n",
        "x_train_seq = tokenizer_text.texts_to_sequences(x_train)\n",
        "x_train_padded = sequence.pad_sequences(x_train_seq, maxlen=200)\n",
        "\n",
        "max_features = None\n",
        "max_features = max_features or len(tokenizer_text.word_index) + 1\n",
        "print(max_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI3AgfKUS4JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer_text.word_index\n",
        "num_words = len(word_index)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words+1, 200))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    if word in embedding_model.index2word:\n",
        "        embedding_matrix[i] = embedding_model[word]\n",
        "        \n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYknxf90TIJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a reverse dictionary\n",
        "reverse_word_map = dict(map(reversed, tokenizer_text.word_index.items()))\n",
        "\n",
        "# Function takes a tokenized sentence and returns the words\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "    return(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fLkfke6aYHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if is_char = True:\n",
        "    char_e = Embedder(char_model_path)\n",
        "    model = ELMoNet(char_e, embedding_matrix, 9)\n",
        "else:\n",
        "    word_e = Embedder(word_model_path)\n",
        "    model = ELMoNet(word_e, embedding_matrix, 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL7s3JelCQF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(index, model, train_dataset, valid_dataset, batchsize):\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=4)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batchsize, shuffle=False, num_workers=4)\n",
        "\n",
        "    no_of_epochs = 50\n",
        "    \n",
        "    valid_loss_min = np.Inf\n",
        "    patience = 4\n",
        "    # current number of epochs, where validation loss didn't increase\n",
        "    p = 0\n",
        "    # whether training should be stopped\n",
        "    stop = False\n",
        "    \n",
        "    since = time.time()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    training_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in tqdm(range(no_of_epochs)):\n",
        "        print('Epoch {}/{}'.format(epoch, no_of_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
        "        for x_batch, y_batch in tk0:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch  = y_batch.to(device)\n",
        "\n",
        "            # Forward Pass\n",
        "            preds = model(x_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Backward Pass and Optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        training_losses.append(epoch_loss)\n",
        "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
        "        \n",
        "        model.eval()\n",
        "        valid_preds = np.zeros((len(valid_dataset), 9))\n",
        "        valid_loss = 0.0\n",
        "        best_val_loss = np.inf\n",
        "        tk1 = tqdm(valid_loader, total=int(len(valid_loader))) \n",
        "        for i, (x_batch, y_batch) in enumerate(tk1):\n",
        "            with torch.no_grad():\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch  = y_batch.to(device)\n",
        "\n",
        "                # Forward Pass\n",
        "                preds = model(x_batch)\n",
        "                loss = criterion(preds, y_batch) \n",
        "            valid_loss += loss.item()\n",
        "            valid_preds[i * valid_loader.batch_size:(i+1) * valid_loader.batch_size, :] = preds.detach().cpu().numpy()\n",
        "        epoch_valid_loss = valid_loss / len(valid_loader)\n",
        "        valid_losses.append(epoch_valid_loss)\n",
        "\n",
        "        if epoch_valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.10f} --> {:.10f}).  Saving model ... to model{}.pt'.format(\n",
        "            valid_loss_min,\n",
        "            epoch_valid_loss,\n",
        "            index))\n",
        "            torch.save(model.state_dict(), f'model{index}.pt')\n",
        "            valid_loss_min = epoch_valid_loss\n",
        "            p = 0\n",
        "\n",
        "        # check if validation loss didn't improve\n",
        "        if epoch_valid_loss > valid_loss_min:\n",
        "            p += 1\n",
        "            print(f'{p} epochs of increasing val loss')\n",
        "            if p > patience:\n",
        "                print('Stopping training')\n",
        "                stop = True\n",
        "                break        \n",
        "\n",
        "        if stop:\n",
        "            break\n",
        "        \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(f'save model => model{index}.bin')\n",
        "    torch.save(model.state_dict(), f'model{index}.bin')\n",
        "    return valid_preds, training_losses, valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEdczhtnouZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_score(y_true, y_pred):\n",
        "    print(\"acc : {}\".format(accuracy_score(y_true, y_pred)))\n",
        "    print(\"f1-score: {}\".format(f1_score(y_true, y_pred, average='weighted')))\n",
        "\n",
        "\n",
        "def run_model(index, model, train_index, val_index):\n",
        "    full_dataset = TextDataset(X=x_train_torch, y=y_train_torch)\n",
        "    train_dataset = torch.utils.data.Subset(full_dataset, train_index)\n",
        "    valid_dataset = torch.utils.data.Subset(full_dataset, val_index)\n",
        "    \n",
        "    batchsize = 64\n",
        "    \n",
        "    valid_preds, training_losses, valid_losses = train_model(index, model, train_dataset, valid_dataset, batchsize)\n",
        "    return valid_preds, training_losses, valid_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMR4lOexouW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_index, val_index = train_test_split(list(x_train.index), test_size=0.2, shuffle=True, random_state=1129, stratify=y_train)\n",
        "y_val = y_train[val_index]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "x_train_torch = torch.tensor(x_train_padded, dtype=torch.long)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
        "model.to(device)\n",
        "print(f\"fold{0} start\")\n",
        "valid_preds, training_losses, valid_losses = run_model(0, model, train_index, val_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5U4pe00wtyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(get_score(np.argmax(valid_preds, 1), np.argmax(y_val, 1)))\n",
        "plt.plot(training_losses)\n",
        "plt.plot(valid_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWGeaVIoQFTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}